------------------------------------
*            Demultiplex           *
------------------------------------

Run interactive session in talaps:
$ srun --account=bgmp --partition=bgmp --nodes=1 --ntasks-per-node=1 --time=1:00:00 --cpus-per-task=1 --pty bash

++++++++++++++++++++++++++++++++++++++++++++++++++++++
+ Part 1 â€“ Quality Score Distribution per-nucleotide +
++++++++++++++++++++++++++++++++++++++++++++++++++++++

------------------------------------------------------
--> 07/26/2022 <--
------------------------------------------------------
Initial data exploration:
used:
$ zcat 1294_S1_L008_R1_001.fastq.gz | less -S
to take a precursory look at the fastq file
Don't do a wc -l on the login node, if we want a line count, do it in an interactive session
the file is more than 300 million reads
$ ls -lah
    total 47G
    drwxrwsr-x+  3 coonrod  bgmp 8.0K Jul 30  2018 .
    drwxrws--x+ 38 sdwagner bgmp 8.0K Jul  1 10:55 ..
    -rw-rwxr--+  1 coonrod  bgmp  20G Jul 30  2018 1294_S1_L008_R1_001.fastq.gz
    -rw-rwxr--+  1 coonrod  bgmp 2.6G Jul 30  2018 1294_S1_L008_R2_001.fastq.gz
    -rw-rwxr--+  1 coonrod  bgmp 2.8G Jul 30  2018 1294_S1_L008_R3_001.fastq.gz
    -rw-rwxr--+  1 coonrod  bgmp  23G Jul 30  2018 1294_S1_L008_R4_001.fastq.gz
    drwx--S---+  2 coonrod  bgmp 8.0K Jul  1 16:08 demultiplexed
    -rwxrwxr-x+  1 sdwagner bgmp  631 Aug  9  2021 indexes.txt
    -rw-rwxr--+  1 coonrod  bgmp  327 Aug 16  2017 README.txt

these are gigantic files. 
to get the read length:
    $ zcat 1294_S1_L008_R1_001.fastq.gz | head -2 | tail -1 | wc
            1       1     102 #include new line character
    $ zcat 1294_S1_L008_R2_001.fastq.gz | head -2 | tail -1 | wc
            1       1     9
    $ zcat 1294_S1_L008_R3_001.fastq.gz | head -2 | tail -1 | wc
            1       1     9
    $ zcat 1294_S1_L008_R4_001.fastq.gz | head -2 | tail -1 | wc
            1       1     102

First read in each file from same location, headers all look the same except for counter [1,4]
Must read all at the same time. 
The indexes / barcodes will be found at the beginning and ends of the line?
is it the barcode itself at the beginning and rev comp at the end?
taking a look at one of the barcodes:
$ zcat 1294_S1_L008_R4_001.fastq.gz | grep "^CACTTCAC"
    CACTTCACCTAGAAAGTGCAAAATTCTTTGTTTTATGAATAAAATATAAGATAACACAAAATGTTTCAGAATGTATAGCCTTTGTTGGTTGGAGATGATTT
    CACTTCACTCTGTACCTGTAGCAAGGTCTGCCAACTCTATGGCTGCCGCACATGCATCTGGCCCTCCCTCTCTGTCCTGGGGTCCTCAGCGTGCTGTCTCC
    CACTTCACGAGAGGACAAATGCCACCTCTCAGCTGCATCTTGAAGCTGGTCACTTCTGGGTCTGTGGCAGGCATTCGTCAATGGCCCAGTGCAGTTCTCTG

i don't see the barcode at the end though...

to test whether it's phred 33 or 64:
$ zcat 1294_S1_L008_R1_001.fastq.gz | sed -n '4~4p' | grep -E "[a-z]+"

this did not return any results, so there are no lower case letters, which would indicate the encoding is 33

Also, # are only in 33, not 64, so it's another one to grep for. 

wrote rev_comp function in both bioinfo.py and python script for qscore Distribution

modified PS4 and PS9 scripts to calculate mean quality scores of a position from fq files
ran python script using sbatch on talapas
i forgot to time the runs. 

these are the runs:
    $ sbatch qdist.srun /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R1_001.fastq.gz 101 6
    Submitted batch job 21725458
    $ sbatch qdist.srun /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R4_001.fastq.gz 101 7
    Submitted batch job 21725461
    $ sbatch qdist.srun /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R2_001.fastq.gz 8 8
    Submitted batch job 21725462
    $ sbatch qdist.srun /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R3_001.fastq.gz 8 9
    Submitted batch job 21725463

    #!/bin/bash
    #SBATCH --partition=bgmp            ### Partition (like a queue in PBS)
    #SBATCH --job-name=qdist            ### Job Name
    #SBATCH --output=qdist_%j.out        ### File in which to store job output
    #SBATCH --error=qdist_%j.err         ### File in which to store job error messages
    #SBATCH --time=0-02:00:00           ### Wall clock time limit in Days-HH:MM:SS
    #SBATCH --nodes=1                   ### Number of nodes needed for the job
    #SBATCH --ntasks-per-node=1         ### Number of tasks to be launched per Node
    #SBATCH --account=bgmp              ### Account used for job submission

While it's running, moving onto next part to count the number of N in the files
$ zcat /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R1_001.fastq.gz | sed -n '4~4p'| grep -o "N" | grep -c

starting some new runs without 
        #SBATCH --time=0-02:00:00           ### Wall clock time limit in Days-HH:MM:SS
and with the timers and directing .err and .out files into directory output/

both runs of index files finished, with runtimes of around 54 minutes. 
this was job job21725647 and job21725648, which were hist 12 and 13, respectively. 
compared to the other two index runs, hist8 and hist9, they look the same for the correct files. 

the first two bio runs without the timer don't seem to have output a histogram and also seem to have exited the queue. 
too bad i don't know what happened bc the .out and .err are not super helpful :(

luckily, the most recent two bio runs, 21725645 and 21725643 are still going, this is at 5 hrs and 7 minutes now.

------------------------------------------------------
--> 07/27/2022 <--
------------------------------------------------------

the biological portion took 7:39:20 to run. exited without errors. 
changed N counting to run with an sbatch because it kept timing out
this way i can keep track of how long it takes to run, too
here's the first version of the bash script:
        #!/bin/bash
        #SBATCH --partition=bgmp            ### Partition (like a queue in PBS)
        #SBATCH --job-name=countN            ### Job Name
        #SBATCH --output=output/countN_%j.out        ### File in which to store job output
        #SBATCH --error=output/countN_%j.err         ### File in which to store job error messages
        #SBATCH --nodes=1                   ### Number of nodes needed for the job
        #SBATCH --ntasks-per-node=1         ### Number of tasks to be launched per Node
        #SBATCH --account=bgmp              ### Account used for job submission

        conda activate bgmp_py310
        /usr/bin/time -v zcat $1 | sed -n '4~4p'| grep -o "N" | wc

wrote psuedo code with Rachel, Jessica, Justine, Sam, and Lisa

------------------------------------------------------
--> 07/28/2022 <--
------------------------------------------------------
writing unit tests to check for various combinations of conditions
countN still hasn't run properly yet, working on trouble shooting to see if grep is the issue
it seems from the first timer that it took around 11 minutes to run for read files.
took around 3 minutes to run for index files. 

so apparently i'm just supposed to count the indexes that contain Ns. rip. changed grep to -c "N"
it took around 3 minutes for each index file. 

------------------------------------------------------
--> 07/29/2022 <--
------------------------------------------------------
Looking at the current histograms, the y-axis is absolutely wrong. 
it's supposed to be 0 to 40, but instead it goes up to 2 trillion.
since I have some unit test files now, i'll try my code again on that to see if it works.
the issue was i was dividing by number of reads rather than by number of records. 
rerunning:
(base) [kli8@talapas-ln1 Assignment-the-first]$ sbatch qdist.srun /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R1_001.fastq.gz 101 0
Submitted batch job 21758626
(base) [kli8@talapas-ln1 Assignment-the-first]$ sbatch qdist.srun /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R2_001.fastq.gz 8 1
Submitted batch job 21758627
(base) [kli8@talapas-ln1 Assignment-the-first]$ sbatch qdist.srun /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R3_001.fastq.gz 8 2
Submitted batch job 21758628
(base) [kli8@talapas-ln1 Assignment-the-first]$ sbatch qdist.srun /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R4_001.fastq.gz 101 3
Submitted batch job 21758629

it took about 3 minutes for a 10 record index, so for 100s of millions of lines, it'll take... well. a lot longer.
it took 52 minutes for the index runs and 7 hrs and 43 minutes for the read runs.
those times could be a lot better, i've heard other people talk about 2 or 3 hours. 
it'll do for now.

------------------------------------------------------
--> 08/02/2022 <--
------------------------------------------------------
open files at top and not in loops, it's a lot faster. 
check N, then Q Score, bc N will have bad Q Scores by default.


Got some feedback for pseudo code:

Lines 20 to 23 - done
"It looks like you do this twice, first in lines 16-19, then 20-23. I think you could just remove that last block."

Line 19 - done
"I assume you mean the index hopped file?" - for unmatched

Line 6 - done
"Might be more efficient to only check for the Q score cutoff in reads with indexes in the known set, 
so you're not calculating it for every read when invalid indexes are already going to the unknown file."

Lines 32 to 38 - unsure that this is asking
"Is this faster than writing out to the file each loop? (I'm asking bc I don't know, not bc it's wrong!)
Also each time you write out to a file, don't forget to add to the header index1_revcomp(index2)"

Lines 26 to 27
"Is it necessary to have 2 sets? maybe you could call your reverse complement function on your 1st set (forward) 
in your code when you get to the step where you're checking the index2 against the set. Either is fine though."

Lines 6 to 7
"I think you need a counter in this line, and maybe at some other points too."

Line 32  - done
"Looks like this could be a function to collect the records from the files, especially as there are 4 input files!"

Line 7
"Missing the counts of the number of matched, unmatched, and unknown reads: "report the number of read-pairs with 
properly matched indexes (per index-pair), the number of read pairs with index-hopping observed, and the number of 
read-pairs with unknown index(es)." Consider counter variables, and if you'll return the counts or write them to an output file."

to open multiple files at a time, can use this syntax:
with (
    open("a", "w") as a,
    open("b", "w") as b
):

leslie recommends using itertools

some useful functions could be:
takewhile(lambda x: x<5, [1,4,6,4,1]) --> 1 4
so for phred score, it could be 
        takewhile(lambda x: x>=30, phred_arr)
        # breaks when phred arr has qscore less than 30


The "A1", "B1", etc associated with the index is sample ID 
should assign key as index seq and ID as value in dict
for naming the files for end user. 